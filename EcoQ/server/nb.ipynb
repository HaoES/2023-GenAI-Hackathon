{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb00e24ff4a43fb92d68e598ff21dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Using cached llama_cpp_python-0.2.23.tar.gz (8.8 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\oussa\\miniconda3\\envs\\sentitorch\\lib\\site-packages (from llama-cpp-python) (4.8.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\oussa\\miniconda3\\envs\\sentitorch\\lib\\site-packages (from llama-cpp-python) (1.24.3)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.23-cp311-cp311-win_amd64.whl size=1764038 sha256=f0778bbaf9221f6a0faf9f3947cd46089da893f5d2dca295a44daa9066102074\n",
      "  Stored in directory: c:\\users\\oussa\\appdata\\local\\pip\\cache\\wheels\\10\\a4\\8a\\9d04b5efbb2272a2be400a1ac94585ae6cc5327199b46163f7\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.23\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: huggingface-cli <command> [<args>]\n",
      "huggingface-cli: error: argument {env,login,whoami,logout,repo,lfs-enable-largefiles,lfs-multipart-upload,scan-cache,delete-cache}: invalid choice: 'download' (choose from 'env', 'login', 'whoami', 'logout', 'repo', 'lfs-enable-largefiles', 'lfs-multipart-upload', 'scan-cache', 'delete-cache')\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli download \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\" \"mistral-7b-instruct-v0.2.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Requirement already satisfied: llama-cpp-python in c:\\users\\oussa\\miniconda3\\envs\\sentitorch\\lib\\site-packages (0.2.23)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\oussa\\miniconda3\\envs\\sentitorch\\lib\\site-packages (from llama-cpp-python) (4.8.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\oussa\\miniconda3\\envs\\sentitorch\\lib\\site-packages (from llama-cpp-python) (1.24.3)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in c:\\users\\oussa\\miniconda3\\envs\\sentitorch\\lib\\site-packages (from llama-cpp-python) (5.6.3)\n"
     ]
    }
   ],
   "source": [
    "# CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" \n",
    "!pip uninstall llama-cpp-python\n",
    "!set CMAKE_ARGS=-DLLAMA_CUBLAS=on\n",
    "!set FORCE_CMAKE=1\n",
    "# !pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir\n",
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "  model_path=\"./mistral-7b-instruct-v0.2.Q4_K_M.gguf\",  # Download the model file first\n",
    "  n_ctx=32768,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "  n_threads=8,  # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "  n_gpu_layers=35,           \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Hello'\n",
    "\n",
    "output = llm(\n",
    "  f\"<s>[INST] {prompt} [/INST]\", # Prompt\n",
    "  max_tokens=128,  # Generate up to 512 tokens\n",
    "  stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
    "  echo=True        # Whether to echo the prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-06f5da44-ac01-4f6e-a0fb-71e1e686fdc5',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1702750222,\n",
       " 'model': './mistral-7b-instruct-v0.2.Q4_K_M.gguf',\n",
       " 'choices': [{'text': \"<s>[INST] Hello [/INST] Hello! How can I assist you today? If you have any questions or topics you'd like to discuss, feel free to ask and I'll do my best to help. If not, we can chat about anything that interests you! Just let me know. ðŸ˜Š\",\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 10, 'completion_tokens': 56, 'total_tokens': 66}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = Llama(model_path=\"./mistral-7b-instruct-v0.2.Q4_K_M.gguf\", chat_format=\"llama-2\", # Set chat_format according to the model you are using\n",
    "            n_ctx=32768,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "            n_threads=8,  # The number of CPU threads to use, tailor to your system and the resulting performance)  \n",
    "            n_gpu_layers=50)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.create_chat_completion(\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in environmental sciences.\"},\n",
    "        {\"role\": \"user\", \"content\": \"what is climate change?.\"}\n",
    "    ],\n",
    "    max_tokens = 256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Climate change refers to long-term modifications in temperatures and weather '\n",
      " 'patterns in a particular region or globally. It is caused primarily by human '\n",
      " 'activities, particularly the emission of greenhouse gases like carbon '\n",
      " \"dioxide and methane, which trap heat in the Earth's atmosphere leading to \"\n",
      " 'rising global temperatures. Other factors contributing to climate change '\n",
      " 'include deforestation, agricultural practices, and industrial processes. The '\n",
      " 'consequences of climate change can be severe and far-reaching, including '\n",
      " 'more frequent and intense extreme weather events, sea level rise, melting '\n",
      " 'glaciers, and negative impacts on ecosystems and human health. It is a '\n",
      " 'complex issue that requires urgent attention and action from individuals, '\n",
      " 'governments, and organizations to mitigate its effects and adapt to the '\n",
      " 'changes already underway.')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(output['choices'][0]['message']['content'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Water scarcity refers to a situation where there is not enough water to meet '\n",
      " 'the needs of a population or an ecosystem. This can be due to various '\n",
      " 'reasons such as drought, over-extraction, or poor management of water '\n",
      " \"resources. In simple terms, water scarcity means that there isn't enough \"\n",
      " 'water available for drinking, farming, industry, and other uses when and '\n",
      " 'where it is needed.')\n"
     ]
    }
   ],
   "source": [
    "output = llm.create_chat_completion(\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in environmental sciences.\"},\n",
    "        {\"role\": \"user\", \"content\": \"define in simple words water scarcity?.\"}\n",
    "    ],\n",
    "    max_tokens = 256\n",
    ")\n",
    "pprint.pprint(output['choices'][0]['message']['content'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'choices': [{'finish_reason': 'stop',\n",
      "              'index': 0,\n",
      "              'message': {'content': ' Water scarcity refers to a situation '\n",
      "                                     'where there is not enough water '\n",
      "                                     'available to meet the needs of people, '\n",
      "                                     'agriculture, industry, or ecosystems. It '\n",
      "                                     'can be caused by natural factors like '\n",
      "                                     'drought or human activities such as '\n",
      "                                     'overuse or pollution. In simple terms, '\n",
      "                                     \"water scarcity means that there isn't \"\n",
      "                                     'enough water for everyone and everything '\n",
      "                                     'that needs it.',\n",
      "                          'role': 'assistant'}}],\n",
      " 'created': 1702573076,\n",
      " 'id': 'chatcmpl-498f7789-e5ad-491d-9a79-095a2cc19780',\n",
      " 'model': './mistral-7b-instruct-v0.2.Q4_K_M.gguf',\n",
      " 'object': 'chat.completion',\n",
      " 'usage': {'completion_tokens': 72, 'prompt_tokens': 34, 'total_tokens': 106}}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentitorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
